"use strict";(self.webpackChunkkaito_website=self.webpackChunkkaito_website||[]).push([[6449],{1781:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"rag-chat-completions-wrapper","title":"RAGEngine Chat Completion Algorithm","description":"The chat_completion method implements a sophisticated filtering and message processing algorithm to determine when to use RAG (Retrieval-Augmented Generation) versus passing requests directly to the LLM. Here\'s how it works:","source":"@site/docs/rag-chat-completions-wrapper.md","sourceDirName":".","slug":"/rag-chat-completions-wrapper","permalink":"/kaito/docs/next/rag-chat-completions-wrapper","draft":false,"unlisted":false,"editUrl":"https://github.com/kaito-project/kaito/tree/main/website/docs/rag-chat-completions-wrapper.md","tags":[],"version":"current","frontMatter":{},"sidebar":"sidebar","previous":{"title":"API Definitions and Examples","permalink":"/kaito/docs/next/rag-api"},"next":{"title":"Custom Model Integration","permalink":"/kaito/docs/next/custom-model"}}');var i=s(74848),o=s(28453);const r={},l="RAGEngine Chat Completion Algorithm",a={},c=[{value:"Algorithm Overview",id:"algorithm-overview",level:2},{value:"Bypass Conditions (Pass-through to LLM)",id:"bypass-conditions-pass-through-to-llm",level:2},{value:"1. No Index Specified",id:"1-no-index-specified",level:3},{value:"2. Tools or Functions Present",id:"2-tools-or-functions-present",level:3},{value:"3. Unsupported Message Roles",id:"3-unsupported-message-roles",level:3},{value:"4. Non-Text Content in User Messages",id:"4-non-text-content-in-user-messages",level:3},{value:"RAG Processing Cases",id:"rag-processing-cases",level:2},{value:"Message Processing Algorithm",id:"message-processing-algorithm",level:3},{value:"Example 1: Simple User Query",id:"example-1-simple-user-query",level:3},{value:"Example 2: Multi-turn Conversation",id:"example-2-multi-turn-conversation",level:3},{value:"Example 3: Multiple Consecutive User Messages",id:"example-3-multiple-consecutive-user-messages",level:3},{value:"Example 4: No User Messages After Assistant",id:"example-4-no-user-messages-after-assistant",level:3},{value:"Token Validation",id:"token-validation",level:2},{value:"Understanding <code>max_tokens</code>",id:"understanding-max_tokens",level:3},{value:"Token Validation Process",id:"token-validation-process",level:3},{value:"1. Context Window Check",id:"1-context-window-check",level:4},{value:"2. Max Tokens Adjustment",id:"2-max-tokens-adjustment",level:4},{value:"Examples of <code>max_tokens</code> Behavior",id:"examples-of-max_tokens-behavior",level:3},{value:"Example 1: No <code>max_tokens</code> Specified",id:"example-1-no-max_tokens-specified",level:4},{value:"Example 2: Conservative <code>max_tokens</code>",id:"example-2-conservative-max_tokens",level:4},{value:"Example 3: Excessive <code>max_tokens</code>",id:"example-3-excessive-max_tokens",level:4},{value:"RAG Execution",id:"rag-execution",level:2},{value:"1. Dynamic Top-K Calculation",id:"1-dynamic-top-k-calculation",level:3},{value:"2. ContextSelectionProcessor: Intelligent Document Filtering",id:"2-contextselectionprocessor-intelligent-document-filtering",level:3},{value:"Token Allocation Strategy",id:"token-allocation-strategy",level:4},{value:"Context Fill Ratio Explained",id:"context-fill-ratio-explained",level:4},{value:"Document Selection Algorithm",id:"document-selection-algorithm",level:4},{value:"3. Chat Engine Execution",id:"3-chat-engine-execution",level:3},{value:"4. Token Budget Example",id:"4-token-budget-example",level:3},{value:"5. Adaptive Quality Features",id:"5-adaptive-quality-features",level:3},{value:"Key Benefits",id:"key-benefits",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"ragengine-chat-completion-algorithm",children:"RAGEngine Chat Completion Algorithm"})}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"chat_completion"})," method implements a sophisticated filtering and message processing algorithm to determine when to use RAG (Retrieval-Augmented Generation) versus passing requests directly to the LLM. Here's how it works:"]}),"\n",(0,i.jsx)(n.h2,{id:"algorithm-overview",children:"Algorithm Overview"}),"\n",(0,i.jsx)(n.p,{children:"The method follows this decision tree:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"1. Validate index existence and parameters\n2. Convert to OpenAI format\n3. Check bypass conditions (no index, tools, unsupported content)\n4. Extract and validate messages\n5. Process messages to separate user prompts from chat history\n6. Validate token limits\n7. Execute RAG-enabled chat completion\n"})}),"\n",(0,i.jsx)(n.h2,{id:"bypass-conditions-pass-through-to-llm",children:"Bypass Conditions (Pass-through to LLM)"}),"\n",(0,i.jsxs)(n.p,{children:["The algorithm will ",(0,i.jsx)(n.strong,{children:"bypass RAG"})," and send requests directly to the LLM in these cases:"]}),"\n",(0,i.jsx)(n.h3,{id:"1-no-index-specified",children:"1. No Index Specified"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n  "model": "gpt-4",\n  "messages": [{"role": "user", "content": "Hello, how are you?"}]\n}\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Result"}),": \u2705 Pass-through (no ",(0,i.jsx)(n.code,{children:"index_name"})," provided)"]}),"\n",(0,i.jsx)(n.h3,{id:"2-tools-or-functions-present",children:"2. Tools or Functions Present"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n  "model": "gpt-4",\n  "index_name": "my_index",\n  "messages": [{"role": "user", "content": "What\'s the weather?"}],\n  "tools": [{"type": "function", "function": {"name": "get_weather"}}]\n}\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Result"}),": \u2705 Pass-through (contains ",(0,i.jsx)(n.code,{children:"tools"}),")"]}),"\n",(0,i.jsx)(n.h3,{id:"3-unsupported-message-roles",children:"3. Unsupported Message Roles"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n  "model": "gpt-4",\n  "index_name": "my_index", \n  "messages": [\n    {"role": "function", "content": "Weather data: 75\xb0F"},\n    {"role": "user", "content": "Thanks!"}\n  ]\n}\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Result"}),": \u2705 Pass-through (",(0,i.jsx)(n.code,{children:"function"})," role not supported for RAG)"]}),"\n",(0,i.jsx)(n.h3,{id:"4-non-text-content-in-user-messages",children:"4. Non-Text Content in User Messages"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n  "model": "gpt-4",\n  "index_name": "my_index",\n  "messages": [\n    {\n      "role": "user", \n      "content": [\n        {"type": "text", "text": "What\'s in this image?"},\n        {"type": "image_url", "image_url": {"url": "data:image/png;base64,..."}}\n      ]\n    }\n  ]\n}\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Result"}),": \u2705 Pass-through (contains image content)"]}),"\n",(0,i.jsx)(n.h2,{id:"rag-processing-cases",children:"RAG Processing Cases"}),"\n",(0,i.jsx)(n.p,{children:"When none of the bypass conditions are met, the algorithm processes messages for RAG:"}),"\n",(0,i.jsx)(n.h3,{id:"message-processing-algorithm",children:"Message Processing Algorithm"}),"\n",(0,i.jsxs)(n.p,{children:["The method processes messages in ",(0,i.jsx)(n.strong,{children:"reverse chronological order"})," to:"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Find all consecutive user messages since the last assistant message"}),"\n",(0,i.jsx)(n.li,{children:"Combine these user messages into a single search query"}),"\n",(0,i.jsx)(n.li,{children:"Keep all other messages as chat history for context"}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Pseudo-code logic:\nfor message in reversed(messages):\n    if message.role == "user" and not assistant_message_found:\n        user_messages_for_prompt.insert(0, message.content)\n    else:\n        if message.role == "assistant":\n            assistant_message_found = True\n        chat_history.insert(0, message)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"example-1-simple-user-query",children:"Example 1: Simple User Query"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n  "model": "gpt-4",\n  "index_name": "docs_index",\n  "messages": [\n    {"role": "system", "content": "You are a helpful assistant."},\n    {"role": "user", "content": "What is KAITO?"}\n  ]\n}\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Processing"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"user_prompt"}),": ",(0,i.jsx)(n.code,{children:'"What is KAITO?"'})," (used for vector search)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"chat_history"}),": ",(0,i.jsx)(n.code,{children:"[system_message]"})," (context for LLM)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Result"}),': \u2705 RAG processing with vector search on "What is KAITO?"']}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"example-2-multi-turn-conversation",children:"Example 2: Multi-turn Conversation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n  "model": "gpt-4", \n  "index_name": "docs_index",\n  "messages": [\n    {"role": "system", "content": "You are a helpful assistant."},\n    {"role": "user", "content": "What is KAITO?"},\n    {"role": "assistant", "content": "KAITO is a Kubernetes operator for AI workloads."},\n    {"role": "user", "content": "How do I install it?"}\n  ]\n}\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Processing"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"user_prompt"}),": ",(0,i.jsx)(n.code,{children:'"How do I install it?"'})," (latest user message for vector search)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"chat_history"}),": ",(0,i.jsx)(n.code,{children:'[system_message, user_message("What is KAITO?"), assistant_message("KAITO is...")]'})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Result"}),": \u2705 RAG processing with vector search on installation question, full conversation context preserved"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"example-3-multiple-consecutive-user-messages",children:"Example 3: Multiple Consecutive User Messages"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n  "model": "gpt-4",\n  "index_name": "docs_index", \n  "messages": [\n    {"role": "system", "content": "You are a helpful assistant."},\n    {"role": "user", "content": "What is KAITO?"},\n    {"role": "assistant", "content": "KAITO is a Kubernetes operator."},\n    {"role": "user", "content": "Tell me more about it."},\n    {"role": "user", "content": "Specifically about GPU support."}\n  ]\n}\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Processing"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"user_prompt"}),": ",(0,i.jsx)(n.code,{children:'"Tell me more about it.\\n\\nSpecifically about GPU support."'})," (combined consecutive user messages)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"chat_history"}),": ",(0,i.jsx)(n.code,{children:'[system_message, user_message("What is KAITO?"), assistant_message("KAITO is...")]'})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Result"}),": \u2705 RAG processing with vector search on combined user query"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"example-4-no-user-messages-after-assistant",children:"Example 4: No User Messages After Assistant"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n  "model": "gpt-4",\n  "index_name": "docs_index",\n  "messages": [\n    {"role": "user", "content": "What is KAITO?"}, \n    {"role": "assistant", "content": "KAITO is a Kubernetes operator."}\n  ]\n}\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Processing"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"user_prompt"}),": ",(0,i.jsx)(n.code,{children:'""'})," (empty - no user messages after assistant)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Result"}),': \u274c Error 400 - "There must be a user prompt since the latest assistant message."']}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"token-validation",children:"Token Validation"}),"\n",(0,i.jsxs)(n.p,{children:["After message processing, the algorithm validates token limits and manages the ",(0,i.jsx)(n.code,{children:"max_tokens"})," parameter:"]}),"\n",(0,i.jsxs)(n.h3,{id:"understanding-max_tokens",children:["Understanding ",(0,i.jsx)(n.code,{children:"max_tokens"})]}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"max_tokens"})," parameter is a standard OpenAI API parameter that controls the maximum number of tokens the model can generate in its response. It serves several important purposes:"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsxs)(n.strong,{children:["Why callers use ",(0,i.jsx)(n.code,{children:"max_tokens"}),":"]})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cost Control"}),": Limit token usage to manage API costs"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Response Length Control"}),": Ensure responses fit within application constraints (UI limits, memory, etc.)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Performance Optimization"}),": Shorter responses generate faster"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Predictable Behavior"}),": Guarantee responses won't exceed expected length"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Default Behavior:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["If ",(0,i.jsx)(n.code,{children:"max_tokens"})," is ",(0,i.jsx)(n.strong,{children:"not specified"}),": The model will generate until it naturally completes the response or hits the context window limit"]}),"\n",(0,i.jsxs)(n.li,{children:["If ",(0,i.jsx)(n.code,{children:"max_tokens"})," is ",(0,i.jsx)(n.strong,{children:"specified"}),": The model will stop generating when it reaches this limit, even if the response is incomplete"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"token-validation-process",children:"Token Validation Process"}),"\n",(0,i.jsx)(n.p,{children:"The algorithm performs two critical validations:"}),"\n",(0,i.jsx)(n.h4,{id:"1-context-window-check",children:"1. Context Window Check"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'if prompt_len > self.llm.metadata.context_window:\n    # Error: Prompt too long - reject request\n    raise HTTPException(status_code=400, detail="Prompt length exceeds context window.")\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"What this validates:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"The entire conversation history + system prompts must fit within the model's context window"}),"\n",(0,i.jsx)(n.li,{children:"If this check fails, the request is rejected entirely (no RAG processing possible)"}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"2-max-tokens-adjustment",children:"2. Max Tokens Adjustment"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'if max_tokens and max_tokens > context_window - prompt_len:\n    # Automatically adjust max_tokens to fit available space\n    logger.warning(f"max_tokens ({max_tokens}) is greater than available context after prompt consideration. Setting to {context_window - prompt_len}.")\n    max_tokens = context_window - prompt_len\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"What this handles:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Ensures ",(0,i.jsx)(n.code,{children:"max_tokens"})," + prompt length doesn't exceed the context window"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Automatically adjusts"})," ",(0,i.jsx)(n.code,{children:"max_tokens"})," downward if needed (with warning logged)"]}),"\n",(0,i.jsx)(n.li,{children:"Prevents context window overflow during RAG execution"}),"\n"]}),"\n",(0,i.jsxs)(n.h3,{id:"examples-of-max_tokens-behavior",children:["Examples of ",(0,i.jsx)(n.code,{children:"max_tokens"})," Behavior"]}),"\n",(0,i.jsxs)(n.h4,{id:"example-1-no-max_tokens-specified",children:["Example 1: No ",(0,i.jsx)(n.code,{children:"max_tokens"})," Specified"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n  "model": "gpt-4",\n  "index_name": "docs_index",\n  "messages": [{"role": "user", "content": "Explain KAITO in detail"}]\n}\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Result"}),": Model generates until natural completion, using a calculated amount of context space for retrieved documents."]}),"\n",(0,i.jsxs)(n.h4,{id:"example-2-conservative-max_tokens",children:["Example 2: Conservative ",(0,i.jsx)(n.code,{children:"max_tokens"})]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n  "model": "gpt-4", \n  "index_name": "docs_index",\n  "messages": [{"role": "user", "content": "Explain KAITO in detail"}],\n  "max_tokens": 100\n}\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Result"}),": Response limited to 100 tokens, more space available for retrieved context, potentially better RAG quality."]}),"\n",(0,i.jsxs)(n.h4,{id:"example-3-excessive-max_tokens",children:["Example 3: Excessive ",(0,i.jsx)(n.code,{children:"max_tokens"})]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n  "model": "gpt-4",\n  "index_name": "docs_index", \n  "messages": [{"role": "user", "content": "Explain KAITO"}],\n  "max_tokens": 8000\n}\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Processing"})," (assuming 8192 context window, 500 token prompt):"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Original"}),": ",(0,i.jsx)(n.code,{children:"max_tokens = 8000"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Available space"}),": ",(0,i.jsx)(n.code,{children:"8192 - 500 = 7692 tokens"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Adjusted"}),": ",(0,i.jsx)(n.code,{children:"max_tokens = 7692"})," (with warning logged)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Result"}),": Automatic adjustment prevents context overflow"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"rag-execution",children:"RAG Execution"}),"\n",(0,i.jsx)(n.p,{children:"The final stage of the algorithm involves sophisticated document retrieval and context management through a multi-layered approach:"}),"\n",(0,i.jsx)(n.h3,{id:"1-dynamic-top-k-calculation",children:"1. Dynamic Top-K Calculation"}),"\n",(0,i.jsx)(n.p,{children:"Before retrieving documents, the system calculates how many documents to fetch from the vector store:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Calculate initial retrieval size based on available context window\ntop_k = max(100, int((context_window - prompt_len) / RAG_DOCUMENT_NODE_TOKEN_APPROXIMATION))\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Key Points:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Minimum"}),": Always retrieves at least 100 documents for good recall"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Token-Based Scaling"}),": Larger available context windows allow more document retrieval"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Approximation Factor"}),": Uses 500 tokens per document node as estimation (configurable via ",(0,i.jsx)(n.code,{children:"RAG_DOCUMENT_NODE_TOKEN_APPROXIMATION"}),")"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Latency Optimization"}),": Calculation considers that FAISS (in-memory) can handle larger retrievals efficiently"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"2-contextselectionprocessor-intelligent-document-filtering",children:"2. ContextSelectionProcessor: Intelligent Document Filtering"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"ContextSelectionProcessor"})," is the core component that intelligently selects which retrieved documents to include in the final prompt. It operates through a sophisticated token management and relevance filtering system:"]}),"\n",(0,i.jsx)(n.h4,{id:"token-allocation-strategy",children:"Token Allocation Strategy"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# 1. Calculate base available tokens\navailable_tokens = context_window - query_tokens - ADDITION_PROMPT_TOKENS\n\n# 2. Apply max_tokens constraint (if specified)\navailable_tokens = min(max_tokens or context_window, available_tokens)\n\n# 3. Apply context fill ratio\nfinal_context_tokens = int(available_tokens * context_token_ratio)\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Token Management Layers:"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Query Token Calculation"}),": Uses actual LLM token counting for the user query"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Buffer Management"}),": Reserves 150 tokens (",(0,i.jsx)(n.code,{children:"ADDITION_PROMPT_TOKENS"}),") for LlamaIndex prompt formatting"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Max Tokens Constraint"}),": Respects the user's ",(0,i.jsx)(n.code,{children:"max_tokens"})," parameter if specified"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Context Fill Ratio"}),": Applies the ",(0,i.jsx)(n.code,{children:"context_token_ratio"})," (default: 0.5, range: 0.2-0.8)"]}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"context-fill-ratio-explained",children:"Context Fill Ratio Explained"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"context_token_ratio"})," parameter controls what percentage of available token space is used for retrieved documents:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n  "context_token_ratio": 0.5,  // Use 50% of available space for context\n  "context_token_ratio": 0.8,  // Use 80% of available space for context (max)\n  "context_token_ratio": 0.2   // Use 20% of available space for context (min)\n}\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Why this matters:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Higher ratios (0.6-0.8)"}),": More context, potentially better answers, but less room for response generation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Lower ratios (0.2-0.4)"}),": Less context, more room for detailed responses"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Balanced approach (0.5)"}),": Default provides good balance between context richness and response space"]}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"document-selection-algorithm",children:"Document Selection Algorithm"}),"\n",(0,i.jsx)(n.p,{children:"The processor applies multiple filters in sequence:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# 1. Sort by relevance (FAISS returns distance scores - lower is better)\nranked_nodes = sorted(nodes, key=lambda x: x.score or 0.0)\n\n# 2. Apply similarity threshold filter (if configured)\nif similarity_threshold and node.score > similarity_threshold:\n    continue  # Skip nodes that don't meet relevance threshold\n\n# 3. Apply token-based selection\nnode_tokens = llm.count_tokens(node.text)\nif node_tokens > available_context_tokens:\n    continue  # Skip nodes that would exceed token budget\n\n# 4. Deduct tokens and include node\navailable_context_tokens -= node_tokens\nresult.append(node)\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Selection Criteria:"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Relevance Ranking"}),": Documents are sorted by similarity score (most relevant first)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Similarity Threshold"}),": Optional filter (default: 0.85) removes low-relevance documents"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Token Fitting"}),": Only includes documents that fit within the calculated token budget"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Greedy Selection"}),": Selects documents in relevance order until token budget is exhausted"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"3-chat-engine-execution",children:"3. Chat Engine Execution"}),"\n",(0,i.jsx)(n.p,{children:"Finally, the system executes the RAG-enabled chat completion:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"chat_engine = index.as_chat_engine(\n    llm=self.llm,\n    similarity_top_k=top_k,           # Initial retrieval size\n    chat_mode=ChatMode.CONTEXT,      # Context-only mode (no query condensation)\n    node_postprocessors=[\n        ContextSelectionProcessor(\n            rag_context_token_fill_ratio=context_token_ratio,\n            llm=self.llm,\n            max_tokens=max_tokens,\n            similarity_threshold=0.85,\n        )\n    ],\n)\n\n# Execute with separated concerns\nchat_result = await chat_engine.achat(user_prompt, chat_history=chat_history)\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Execution Flow:"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Vector Search"}),": ",(0,i.jsx)(n.code,{children:"user_prompt"})," is used for semantic search against the index"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Initial Retrieval"}),": Fetches ",(0,i.jsx)(n.code,{children:"top_k"})," most similar documents from vector store"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Context Selection"}),": ",(0,i.jsx)(n.code,{children:"ContextSelectionProcessor"})," filters down to final document set"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Prompt Construction"}),": Selected documents are formatted into the final prompt"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"LLM Generation"}),": Complete prompt (context + history + user query) sent to LLM"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"In the event that no relevant context is found, the LlamaIndex library will return an empty response. We look out for this and send the request directly to the LLM without context as described in the bypass handling above."})}),"\n",(0,i.jsx)(n.h3,{id:"4-token-budget-example",children:"4. Token Budget Example"}),"\n",(0,i.jsx)(n.p,{children:"Here's how token allocation works in practice:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Scenario: 8192 context window, 500 token conversation, max_tokens=1000, context_token_ratio=0.6\n\n1. Base calculation:\n   Available = 8192 - 500 (conversation) - 150 (formatting) = 7542 tokens\n\n2. Apply max_tokens constraint:\n   Available = min(1000, 7542) = 1000 tokens\n\n3. Apply context ratio:\n   Context budget = 1000 * 0.6 = 600 tokens for retrieved documents\n   Response budget = 1000 - 600 = 400 tokens for LLM response\n\n4. Document selection:\n   - Retrieve top_k documents (100 as a min)\n   - ContextSelectionProcessor selects best documents fitting in 600 tokens\n   - Might end up with 3-4 high-quality documents depending on their length\n"})}),"\n",(0,i.jsx)(n.h3,{id:"5-adaptive-quality-features",children:"5. Adaptive Quality Features"}),"\n",(0,i.jsx)(n.p,{children:"The system includes several adaptive features for optimal performance:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Zero-Context Handling"}),": If no tokens are available for context, gracefully continues with just the conversation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Large Document Handling"}),": Automatically skips documents that are too large for the token budget"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Relevance Filtering"}),": Similarity threshold prevents inclusion of irrelevant documents"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Logging and Monitoring"}),": Detailed logs show selection decisions for debugging"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"This approach ensures that RAG execution is both intelligent and efficient, maximizing the quality of retrieved context while respecting all token constraints and user preferences."}),"\n",(0,i.jsx)(n.h2,{id:"key-benefits",children:"Key Benefits"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Intelligent Request Routing"}),": Automatically determines when to use RAG vs. direct LLM calls based on content type, tools, and message structure"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Smart Message Processing"}),": Extracts recent user queries for vector search while preserving full conversation context"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Adaptive Token Management"}),": Dynamic context allocation with configurable ratios and automatic ",(0,i.jsx)(n.code,{children:"max_tokens"})," adjustment"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Precision Document Selection"}),": Multi-layered filtering combining relevance scoring, similarity thresholds, and token budgets"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Graceful Degradation"}),": Handles edge cases (no context space, oversized documents) without failure"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"This algorithm maximizes RAG effectiveness while maintaining OpenAI API compatibility and robust error handling."})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},28453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>l});var t=s(96540);const i={},o=t.createContext(i);function r(e){const n=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);